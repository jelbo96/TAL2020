{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 - Modelos de lenguaje (N-Grama) y Word Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modelos de lenguaje\n",
    "\n",
    "### 1.1 Motivación\n",
    "\n",
    "En el tutorial 1, vimos que el enfoque **bag-of-words** tiende a considerar las palabras como variables independientes. En otras palabras, este enfoque ingenuo considera que la aparición de una serie de palabras no impacta las probabilidades de cuáles serán la o las palabras siguientes.\n",
    "\n",
    "<i>Ejemplo</i>: \"El gato come sus ...\"\n",
    "\n",
    "La motivación de los **modelos de lenguaje** es dotar la máquina de una mejor comprensión del lenguaje representando las relaciones probabilísticas entre las palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Aplicaciones de los modelos de lenguaje\n",
    "\n",
    "- En <u>Traducción automática</u>, permite evaluar que tan probables son varias posibilidades de traducción. En el ejemplo, podría indicar que la posibilidad 1 es más probable.\n",
    "\n",
    "<i>Ejemplo</i>: \"El gato del dueño de la casa se come sus croquetas\"\n",
    "1) The house owner's cat eats his kibble \n",
    "2) The cat of the owner of the house eats his kible\n",
    "\n",
    "- En <u>Corrección automática</u>, permite détectar errores probables. En el ejemplo, aunque la palabra \"dueña\" es correcta, es poco probable que aparezca después de la palabra \"del\".\n",
    "\n",
    "<i>Ejemplo</i>: \"El gato del dueña de la casa\"\n",
    "\n",
    "- En <u>Finalización automática de textos</u>, permite hacer sugerencias de cómo seguir un texto.\n",
    "\n",
    "![Finalizacion de textos](T2-languagemodel.png \"Logo Title Text 1\")\n",
    "\n",
    "- En <u> Identificación automática del autor</u>, <u> Generación automática de texto</u>, <u> Resumen automático</u> y muchas otras aplicaciones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Definición\n",
    "\n",
    "Un modelo de lenguaje es una distribución de probabilidades sobre secuencias de palabras: P($w_1$, ...., $w_n$), donde $w_k$ son las palabras de la secuencia y $w_1$,...,$w_n$ la secuencia completa.\n",
    "\n",
    "\n",
    "- Existe varias maneras de calcular estas probabilidades. Podemos distinguir los modelos de languaje **n-grama** (aprendidos con métodos estadísticos estandares) y los modelos de lenguaje **neuronales** (aprendidos con redes neuronales).\n",
    "\n",
    "\n",
    "- Una manera común de calcular P($w_1$, ...., $w_n$) con métodos estadísticos estándares:\n",
    "\n",
    "P($w_1$) P($w_2$|$w_1$) P($w_3$|$w_1,w_2$) P($w_4$|$w_1,...,w_3$) P($w_n$|$w_1,...,w_{n-1}$) (<i>chain rule</i> en probabilidades)\n",
    "\n",
    "- En práctica, se utiliza el concepto de **modelo de N-grama** (por ejemplo N=2 o N=3) para referirse al tamaño máximo de las secuencias que se consideran en el modelo de lenguaje.\n",
    "\n",
    "### 1.4 Aprender un modelo de lenguaje N-Grama\n",
    "\n",
    "- Modelo bigrama (N=2)\n",
    "\n",
    "P($w_{n-1}$, $w_n$) = $\\frac{C(w_{n-1}, w_n)}{C(w_{n-1})}$\n",
    "\n",
    "donde C corresponde a contar cuántas veces aparece cierta secuencia en un dataset (o corpus) de entrenamiento.\n",
    "\n",
    "- Caso general (cualquier valor de N)\n",
    "\n",
    "P($w_{n-N+1}$,...,$w_n$) = $\\frac{C(w_{n-N+1,...,n-1}, w_n)}{C(w_{n-N+1,...,n-1})}$\n",
    "\n",
    "- Ejemplo:\n",
    "\n",
    "Corpus: \n",
    "\n",
    "1. (s) I am Sam (/s)\n",
    "2. (s) Sam I am (/s)\n",
    "3. (s) I do not like eggs (/s)\n",
    "\n",
    "Modelo de lenguaje bigrama (muestra):\n",
    "\n",
    "1. P(I | (s)) = 2/3\n",
    "2. P((/s) | Sam) = 1/2\n",
    "\n",
    "- Calcular la probabilidad de una frase \"El gato come croquetas.\" con un modelo de lenguaje bigrama\n",
    "\n",
    "$P(gato | el) P(come | gato) P(croquetas | come) P((/s) | croquetas)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Un ejemplo práctico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip: orden no encontrada\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /home/jelbo/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/jelbo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprender un modelo de lenguaje trigrama (N=3) en el dataset \"Reuters\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Create a placeholder for model\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "# Count frequency of co-occurance  \n",
    "for sentence in reuters.sents():\n",
    "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "        model[(w1, w2)][w3] += 1\n",
    " \n",
    "# Let's transform the counts to probabilities\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'told',\n",
       " 'Reuter',\n",
       " 'correspondents',\n",
       " 'in',\n",
       " 'Asian',\n",
       " 'capitals',\n",
       " 'a',\n",
       " 'U',\n",
       " '.',\n",
       " 'S',\n",
       " '.',\n",
       " 'Move',\n",
       " 'against',\n",
       " 'Japan',\n",
       " 'might',\n",
       " 'boost',\n",
       " 'protectionist',\n",
       " 'sentiment',\n",
       " 'in',\n",
       " 'the',\n",
       " 'U',\n",
       " '.',\n",
       " 'S',\n",
       " '.',\n",
       " 'And',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'curbs',\n",
       " 'on',\n",
       " 'American',\n",
       " 'imports',\n",
       " 'of',\n",
       " 'their',\n",
       " 'products',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.sents()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizar algunas predicciones con el modelo de lenguaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'competitive': 0.03225806451612903,\n",
       " 'positives': 0.03225806451612903,\n",
       " 'flexible': 0.03225806451612903,\n",
       " 'likely': 0.03225806451612903,\n",
       " 'favourable': 0.03225806451612903,\n",
       " 'broadly': 0.03225806451612903,\n",
       " 'favorable': 0.03225806451612903,\n",
       " 'appropriately': 0.03225806451612903,\n",
       " 'effective': 0.06451612903225806,\n",
       " 'sensitive': 0.03225806451612903,\n",
       " 'than': 0.12903225806451613,\n",
       " 'anxious': 0.06451612903225806,\n",
       " 'expensive': 0.03225806451612903,\n",
       " 'interested': 0.03225806451612903,\n",
       " 'specific': 0.03225806451612903,\n",
       " 'pessimistic': 0.0967741935483871,\n",
       " 'profitable': 0.06451612903225806,\n",
       " 'important': 0.03225806451612903,\n",
       " 'developed': 0.03225806451612903,\n",
       " 'realistic': 0.06451612903225806,\n",
       " 'consistent': 0.03225806451612903,\n",
       " 'promising': 0.03225806451612903}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(model[\"are\",\"more\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yesterday': 0.004651162790697674,\n",
       " 'of': 0.3209302325581395,\n",
       " 'it': 0.05581395348837209,\n",
       " 'effect': 0.004651162790697674,\n",
       " 'cut': 0.009302325581395349,\n",
       " 'for': 0.05116279069767442,\n",
       " 'paid': 0.013953488372093023,\n",
       " 'to': 0.05581395348837209,\n",
       " 'increases': 0.013953488372093023,\n",
       " 'used': 0.004651162790697674,\n",
       " 'climate': 0.004651162790697674,\n",
       " '.': 0.023255813953488372,\n",
       " 'cuts': 0.009302325581395349,\n",
       " 'reductions': 0.004651162790697674,\n",
       " 'limit': 0.004651162790697674,\n",
       " 'now': 0.004651162790697674,\n",
       " 'moved': 0.004651162790697674,\n",
       " 'per': 0.013953488372093023,\n",
       " 'adjustments': 0.004651162790697674,\n",
       " '(': 0.009302325581395349,\n",
       " 'slumped': 0.004651162790697674,\n",
       " 'is': 0.018604651162790697,\n",
       " 'move': 0.004651162790697674,\n",
       " 'evolution': 0.004651162790697674,\n",
       " 'differentials': 0.009302325581395349,\n",
       " 'went': 0.004651162790697674,\n",
       " 'the': 0.013953488372093023,\n",
       " 'factor': 0.004651162790697674,\n",
       " 'Royal': 0.004651162790697674,\n",
       " ',': 0.018604651162790697,\n",
       " 'again': 0.004651162790697674,\n",
       " 'changes': 0.004651162790697674,\n",
       " 'holds': 0.004651162790697674,\n",
       " 'has': 0.009302325581395349,\n",
       " 'fall': 0.004651162790697674,\n",
       " '-': 0.004651162790697674,\n",
       " 'from': 0.004651162790697674,\n",
       " 'base': 0.004651162790697674,\n",
       " 'on': 0.004651162790697674,\n",
       " 'review': 0.004651162790697674,\n",
       " 'while': 0.004651162790697674,\n",
       " 'collapse': 0.004651162790697674,\n",
       " 'being': 0.004651162790697674,\n",
       " 'at': 0.023255813953488372,\n",
       " 'outlook': 0.004651162790697674,\n",
       " 'rises': 0.004651162790697674,\n",
       " 'drop': 0.004651162790697674,\n",
       " 'guaranteed': 0.004651162790697674,\n",
       " ',\"': 0.004651162790697674,\n",
       " 'stayed': 0.009302325581395349,\n",
       " 'structure': 0.004651162790697674,\n",
       " 'and': 0.004651162790697674,\n",
       " 'could': 0.004651162790697674,\n",
       " 'related': 0.004651162790697674,\n",
       " 'hike': 0.004651162790697674,\n",
       " 'we': 0.004651162790697674,\n",
       " 'adjustment': 0.023255813953488372,\n",
       " 'policy': 0.004651162790697674,\n",
       " 'was': 0.009302325581395349,\n",
       " 'revision': 0.004651162790697674,\n",
       " 'freeze': 0.009302325581395349,\n",
       " 'led': 0.004651162790697674,\n",
       " 'action': 0.004651162790697674,\n",
       " 'zone': 0.004651162790697674,\n",
       " 'slump': 0.004651162790697674,\n",
       " 'had': 0.004651162790697674,\n",
       " 'difference': 0.004651162790697674,\n",
       " 'in': 0.004651162790697674,\n",
       " 'raise': 0.004651162790697674,\n",
       " 'increase': 0.009302325581395349,\n",
       " 'will': 0.013953488372093023,\n",
       " 'support': 0.004651162790697674,\n",
       " 'gap': 0.004651162790697674,\n",
       " 'would': 0.009302325581395349,\n",
       " 'projected': 0.004651162790697674,\n",
       " 'approached': 0.004651162790697674,\n",
       " 'instability': 0.004651162790697674}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(model[\"the\",\"price\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos iterar sobre el modelo y generar frases aleatorias que parecen coherentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today the Bank of Buckhannon , in January , but we are prepared to negotiate .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# starting words\n",
    "text = [\"today\", \"the\"]\n",
    "sentence_finished = False\n",
    " \n",
    "while not sentence_finished:\n",
    "  # select a random probability threshold  \n",
    "  r = random.random()\n",
    "  accumulator = .0\n",
    "\n",
    "  for word in model[tuple(text[-2:])].keys():\n",
    "      accumulator += model[tuple(text[-2:])][word]\n",
    "      # select words that are above the probability threshold\n",
    "      if accumulator >= r:\n",
    "          text.append(word)\n",
    "          break\n",
    "\n",
    "  if text[-2:] == [None, None]:\n",
    "      sentence_finished = True\n",
    " \n",
    "print (' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Limitaciones de los modelos de lenguaje N-Grama\n",
    "\n",
    "\n",
    "1) Un modelo de lenguaje N-Grama con un valor N mayor es más preciso pero genera problemas de computación.\n",
    "\n",
    "2) Los modelos N-gramas son representaciones escasa/ingenua del lenguaje. Solo consideran la forma de las palabras y no su significado/semántica\n",
    "\n",
    "\n",
    "Para mejorar estas limitaciones:\n",
    "\n",
    "- **Word Embedding** (proyección semántica de las palabras a través de vectores): Word2Vec, GLoVe\n",
    "\n",
    "- **Modelos de lenguaje neuronales**: BERT, GPT-2, GPT-3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Motivación: representar las dimensiones semánticas de cada palabra\n",
    "\n",
    "1. I want an orange juice.\n",
    "2. I want an apple ____ .\n",
    "\n",
    "- Los enfoques <i>bag of words</i> y <i>modelos de lenguaje N-Grama</i> no tienen la capacidad de calcular que las frases 1 y 2 son muy similares porque no tienen una manera de representar que las palabras 'orange' y 'apple' comparten caracterícas (<i>features</i>) comunes.\n",
    "\n",
    "Los enfoques ingenuos tieden a representar las palabras como vectores \"1-Hot\". Por ejemplo, supongamos que tenemos un vocabulario de sólo cinco palabras: King, Queen, Man, Woman y Child. Se codificaría la palabra 'Queen' como:\n",
    "\n",
    "<img src=\"img/word2vec1.png\"/>\n",
    "\n",
    "- Sería más interesante poder representar la semántica de cada palabra tomando en cuentas ciertas características. \n",
    "\n",
    "<img src=\"img/word2vec2.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Definición\n",
    "\n",
    "El concepto de **word embedding** se refiere a un conjunto de técnicas utilizadas para aprender representaciones matemáticas, tipicamente vectores, de cada palabra.\n",
    "\n",
    "Una de las técnicas más populares es __Word2Vec__ propuesto por un equipo de investigación de Google en 2013 (Efficient Estimation of Word Representations in Vector Space [Mikolov et al., 2013]).\n",
    "\n",
    "Alternativas populares son __GloVe__ (propuesta por la Universidad de Stanford en 2014) y __FastText__ (propuesta por Facebook en 2016), que extende Word2Vec para considerar de mejor manera las palabras con errores ortográficas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Algunas propiedades de los word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tener representaciones vectoriales de las palabras permite calcular \"razonamiento\" de tipo __King - Man + Woman = ?__ y llegar a un resultado cerca de __Queen__.\n",
    "\n",
    "<img src=\"img/word2vec4.png\"/>\n",
    "\n",
    "- Tener representaciones vectoriales de las palabras permite realizar razonamientos analógicos de tipo __A es a B, lo que C es a ..__ . Este tipo de propiedades es muy útil para aplicaciones de _Question Answering_ por ejemplo. Las respuestas a las pregutas siguientes <i>¿Cuál es la capital de Chile?</i> o <i>¿Cuáles son los clubs de fútbol en Chile?</i> se pueden responder adicionando vectores.\n",
    "\n",
    "<img src=\"img/word2vec6.png\"/>\n",
    "\n",
    "<img src=\"img/word2vec7.png\"/>\n",
    "\n",
    "<img src=\"img/word2vec8.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 ¿Cómo se aprenden los vectores? - Redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ara construir sus vectores, Word2Vec utiliza un dataset de entrenamiento y algoritmos de aprendizaje basados en redes neuronales (__Continuous Bag of Words__ (CBOW), o modelo __Skip Gram__). El objetivo de esta fase de aprendizaje es aprender cuáles son las palabras _X_ más probables de aparecer en el contexto de una palabra _y_.\n",
    "\n",
    "<img src=\"img/word2vec5.png\"/>\n",
    "\n",
    "Por ejemplo, ¿cuál es la probabilidad de tener la palabra 'perro' si aparece la palabra 'pelota' en el contexto?\n",
    "\n",
    "<code>Los expertos explican que los __perros__ persiguen __pelotas__ en movimiento como parte de un comportamiento instintivo. Aunque no todos los perros tienen tan despiertos su instinto de caza, esto no impide que la mayoría de ellos sí disfruten, y mucho, de los juegos que incluyen persecuciones de una saltarina __pelota__ que bota delante de ellos. </code>\n",
    "\n",
    "__Algoritmo CBOW__\n",
    "\n",
    "Las palabras de contexto forman la capa de entrada. Si el tamaño del vocabulario es V, estos serán vectores de dimensión V con sólo uno de los elementos establecido en uno, y el resto todos los ceros. Hay una sola capa oculta y una capa de salida.\n",
    "\n",
    "<img src=\"img/word2vec9.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Un ejemplo práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase <code>word2vec</code> de Gensim permite word embeddings de palabras (ver documentación: https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "Esta clase tiene varios parametros, en particular:\n",
    "- <code>sentences</code>: una lista de palabras o de frases que sirve para entrenar el modelo\n",
    "- <code>sg</code>: define que algoritmos de aprendizaje utilizar (0=CBOW, 1=skip-gram)\n",
    "- <code>size</code>: define la dimensión de los vectores que se desea extraer\n",
    "- <code>window</code>: define el número de palabras considerar a la izquierda y a la derecha de una palabra\n",
    "- <code>min_count</code>: ignorar las palabras que aparecen menos de _min_count_\n",
    "y otros asociados a la parametrización de la fase de aprendizaje de la red neuronal (que no detallaremos en esta parte del curso):\n",
    "- <code>alpha</code>: el _learning rate_ utilizado para optimizar los parametros de la red neuronal.\n",
    "- <code>iter</code>: número de iteraciones (epocas) sobre el dataset para encontrar los parametreos que optimizan la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar nuestro modelo Word2Vec, podemos utilizar nuestros propios datasets o utilizar datasets genericos existentes. Para empezar, utilizaremos 100 MB de textos extraidos de Wikipedia en inglés, para generar vectores de 200 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.Text8Corpus('datasets/text8.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Text8Corpus at 0x7f38b4c89100>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences,size=200,hs=1)\n",
    "#model=word2vec.Word2Vec.load(\"text8_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=71290, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos aprendido nuestro modelo, tratemos de resolver la ecuación <code>King - Man + Woman</code>.\n",
    "\n",
    "En otras palabras buscamos cuál es el vector más similar al vector que adiciona positivamente 'King' y 'Woman' y negativamente 'Man'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('throne', 0.5472026467323303),\n",
       " ('prince', 0.544721245765686),\n",
       " ('emperor', 0.5333271622657776),\n",
       " ('empress', 0.5315308570861816),\n",
       " ('queen', 0.5310486555099487)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman','king'],negative=['man'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conflicts', 0.7076013088226318),\n",
       " ('clashes', 0.7021306753158569),\n",
       " ('confrontation', 0.6600365042686462),\n",
       " ('struggle', 0.6469467878341675),\n",
       " ('hostilities', 0.6333697438240051),\n",
       " ('tensions', 0.6308213472366333),\n",
       " ('confrontations', 0.6173384189605713),\n",
       " ('dispute', 0.5881558656692505),\n",
       " ('strife', 0.5749542117118835),\n",
       " ('disputes', 0.5740856528282166)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"conflict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('confrontation', 0.6045141220092773),\n",
       " ('warfare', 0.5797349810600281),\n",
       " ('fighting', 0.5483338832855225),\n",
       " ('weapons', 0.5354194641113281),\n",
       " ('combat', 0.5192270278930664),\n",
       " ('struggle', 0.5127138495445251),\n",
       " ('conflicts', 0.509809136390686),\n",
       " ('pistol', 0.5049012899398804),\n",
       " ('rifle', 0.4946534335613251),\n",
       " ('threat', 0.49184662103652954)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"conflict\",\"weapon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clashes', 0.5165125727653503),\n",
       " ('conflicts', 0.491607666015625),\n",
       " ('tensions', 0.45207974314689636),\n",
       " ('intermarriage', 0.44213706254959106),\n",
       " ('disputes', 0.433108389377594),\n",
       " ('disagreements', 0.4293721914291382),\n",
       " ('negotiations', 0.4285081624984741),\n",
       " ('hostilities', 0.4215063154697418),\n",
       " ('frictions', 0.415609747171402),\n",
       " ('antagonism', 0.4116770625114441)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"conflict\"],negative=[\"weapon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('childhood', 0.5526549220085144),\n",
       " ('lives', 0.5165643095970154),\n",
       " ('career', 0.4972711503505707),\n",
       " ('teens', 0.47553524374961853),\n",
       " ('experiences', 0.4677225947380066),\n",
       " ('universe', 0.4317152202129364),\n",
       " ('humanity', 0.4283400774002075),\n",
       " ('mankind', 0.4277868866920471),\n",
       " ('intellect', 0.42645975947380066),\n",
       " ('adolescence', 0.42217469215393066)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"life\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('childhood', 0.4200892150402069),\n",
       " ('experiences', 0.3805443048477173),\n",
       " ('lives', 0.3487817049026489),\n",
       " ('incarnations', 0.3481512665748596),\n",
       " ('intellect', 0.338698148727417),\n",
       " ('adolescence', 0.3299098312854767),\n",
       " ('reincarnation', 0.3219226598739624),\n",
       " ('career', 0.32055985927581787),\n",
       " ('rebirth', 0.31949448585510254),\n",
       " ('conception', 0.31615379452705383)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"life\"],negative=[\"money\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver los parametros aprendidos por la red neuronal para una palabra dada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2513365e-01,  3.4834024e-01, -8.7005490e-01, -1.0518003e+00,\n",
       "        2.3424676e-01,  8.6906239e-02, -1.4051829e-01, -1.3161856e+00,\n",
       "        1.1991868e+00, -7.7854699e-01, -3.5599831e-01, -9.4918340e-01,\n",
       "        6.2663370e-01, -1.7075832e+00, -2.3364359e-01, -7.2287303e-01,\n",
       "       -4.7524503e-01,  3.6752862e-01,  3.4962529e-01, -1.7126049e+00,\n",
       "        7.4232823e-01, -1.4010091e+00, -2.5057253e-01,  1.1530570e+00,\n",
       "        9.6159124e-01,  1.1286738e+00, -3.8628101e-01, -6.3577342e-01,\n",
       "        1.5322101e+00,  4.1410491e-01,  9.4766921e-01, -4.0340963e-01,\n",
       "        7.7060267e-02, -6.2588319e-02, -2.1098179e-01,  1.0037588e+00,\n",
       "       -1.0997274e+00, -2.8602829e-02, -1.4404228e+00,  3.7883124e-01,\n",
       "        4.5026857e-01, -7.3347986e-01, -4.4738498e-01,  1.7258023e+00,\n",
       "       -8.6186337e-01, -1.1202334e-01, -3.5885575e-01,  2.3931946e-01,\n",
       "        1.2869003e-01,  3.4474808e-01,  2.8526005e-01,  6.1115551e-01,\n",
       "        9.8589867e-01,  1.2098290e+00, -1.7157161e-01, -1.3830528e-02,\n",
       "        7.1394241e-01, -8.3902800e-01, -6.1239835e-02,  1.1089885e-01,\n",
       "        5.1206833e-01,  4.5497584e-01,  1.8324770e-02, -9.7938907e-01,\n",
       "       -6.7276025e-01,  3.1242618e-01, -2.5540836e-02,  9.3330854e-01,\n",
       "       -1.4749802e+00, -1.9694783e-01, -1.2525858e+00, -8.9043653e-01,\n",
       "        3.4573138e-01,  1.6849725e+00,  1.7243467e-01,  5.5317137e-02,\n",
       "        8.0751646e-01, -1.8344569e+00, -1.0703394e+00, -1.7868218e+00,\n",
       "        5.0734615e-01,  2.9407132e-01,  2.2823720e+00, -1.2418759e+00,\n",
       "        2.0869517e-01,  6.1324972e-01, -3.1217197e-01, -1.1223508e+00,\n",
       "        1.3547863e+00,  1.1007843e+00,  1.7841669e-02, -4.9498621e-01,\n",
       "        8.0581784e-01, -6.3285798e-01, -1.3549753e+00,  7.6348728e-01,\n",
       "        6.4990360e-01, -4.5196530e-01,  1.4768904e+00, -8.5724831e-01,\n",
       "        2.3588960e+00, -6.5226948e-01, -3.7605891e-01, -1.4702270e+00,\n",
       "       -2.4884531e-01,  8.4508437e-01, -3.6014882e-03, -4.3058878e-01,\n",
       "        1.3279569e+00, -9.2681867e-01,  6.0650700e-01, -2.6703426e-01,\n",
       "        8.1211813e-02, -1.5712310e-01,  2.4048679e-01, -8.9077169e-01,\n",
       "        5.2767122e-01, -1.2170222e+00, -1.4127269e+00, -6.3705695e-01,\n",
       "        9.9827528e-01, -4.1367199e-02,  8.8465232e-01, -4.4041511e-02,\n",
       "        1.4541491e+00,  7.8538084e-01, -8.9503624e-02, -4.0713823e-01,\n",
       "        1.3342178e+00, -1.2494875e+00,  3.7229165e-01, -1.4153391e+00,\n",
       "        1.0424119e+00,  3.9629418e-02, -5.5785868e-02,  5.1191598e-01,\n",
       "       -7.4803960e-01, -6.6906530e-01,  1.4763936e+00,  2.1228793e-01,\n",
       "       -1.0405278e+00, -3.3432525e-01,  1.3052665e-01,  1.8656384e+00,\n",
       "        3.9768720e-01, -1.3215641e+00, -8.6165413e-02,  6.1865968e-01,\n",
       "       -1.1788734e+00, -1.5423710e+00, -1.9148134e+00,  3.4899464e-01,\n",
       "       -5.6031448e-01, -8.3707732e-01, -2.5789869e-01,  9.6238446e-01,\n",
       "        9.8152190e-01,  1.3021373e+00,  1.0915080e-01, -4.7719833e-01,\n",
       "       -1.5924302e+00, -5.3578037e-01,  1.3588072e-01,  5.7555046e-02,\n",
       "        9.0005910e-01,  2.1410930e+00,  7.0742911e-01, -1.5311366e-01,\n",
       "        7.4375570e-01,  5.7993597e-01,  2.4664527e-01,  6.2528032e-01,\n",
       "       -2.7096334e-03, -1.8432876e+00,  2.1677773e-01,  5.6942856e-01,\n",
       "       -7.7245539e-01, -8.1510669e-01,  1.6947632e-01, -6.4959499e-04,\n",
       "        4.7164550e-01,  5.0541949e-01, -1.6711075e+00,  2.2753152e-01,\n",
       "        9.7180009e-01,  2.6571753e-02,  4.9483231e-01,  5.1010913e-01,\n",
       "       -6.9296944e-01, -2.2058319e-01,  5.1179969e-01,  1.6619914e+00,\n",
       "        8.1426680e-01, -4.2658079e-01, -1.6391457e+00, -9.9899149e-01,\n",
       "        1.7902007e+00, -8.3869529e-01, -1.2341278e+00,  5.2569443e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"text8_model\")\n",
    "model=word2vec.Word2Vec.load(\"text8_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jelbo/.local/lib/python3.8/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"brazil chile france peru argentina\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hammer'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"apple pear banana hammer\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6331564"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20868663"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','hammer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14998363"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('woman','hammer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10004818"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047085017"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('woman','engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29920813"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('man','baby')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4618712"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('woman','baby')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Limitaciones de los word embeddings\n",
    "\n",
    "Las técnicas de Word Embeddings dan resultados muy interesantes pero tienen dos principales limitaciones:\n",
    "\n",
    "1) No permiten tomar en cuenta el orden entre las palabras.\n",
    "\n",
    "Ejemplo: \"Estamos aqui para trabajar y no jugar\" vs. \"Estamos aqui para jugar y no trabajar\"\n",
    "\n",
    "2) No permiten tomar en cuenta que ciertas palabras cambian de significado según el contexto.\n",
    "\n",
    "Ejemplo: \"I lost my computer __mouse__\"\n",
    "\n",
    "Para mejorar estas limitaciones:\n",
    "\n",
    "- Combinar Word Embedding con redes neuronales (convolucionales (CNN) o secuenciales (RNN)) que toman en cuenta el orden entre las palabras\n",
    "\n",
    "- Utilizar modelos de lenguaje neuronales que toman en cuenta el contexto de las palabras: BERT, GPT-2, GPT-3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Trabajo práctico\n",
    "\n",
    "1) Aprender modelos de lenguaje N-Grama (N=3, N=4 o N=5) para distintos medios de prensa\n",
    "\n",
    "- ¿Se puede observar algunas diferencias relevantes en los modelos de lenguaje de cada medio?\n",
    "- ¿Se podría identificar sesgos ideológicos utilizando estos modelos de lenguaje?\n",
    "\n",
    "2) Aprender distintos <i>word embeddings</i> utilizando distintos medios de prensa como datasets de entrenamiento\n",
    "\n",
    "- ¿Se puede observar algunas diferencias relevantes?\n",
    "- ¿Se podría identificar sesgos ideológicos utilizando estos word embeddings?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-16</td>\n",
       "      <td>Longer jail terms for serious offenders, commu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-15</td>\n",
       "      <td>Video report by ITV News Political Corresponde...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-15</td>\n",
       "      <td>Significant further restrictions on our freedo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-15</td>\n",
       "      <td>Labour leader Sir Keir Starmer will not take p...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-15</td>\n",
       "      <td>A new \"rule of six\" restriction is being imple...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                                               text Unnamed: 2\n",
       "0  2020-09-16  Longer jail terms for serious offenders, commu...        NaN\n",
       "1  2020-09-15  Video report by ITV News Political Corresponde...        NaN\n",
       "2  2020-09-15  Significant further restrictions on our freedo...        NaN\n",
       "3  2020-09-15  Labour leader Sir Keir Starmer will not take p...        NaN\n",
       "4  2020-09-15  A new \"rule of six\" restriction is being imple...        NaN"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET=\"datasets/itv-reinounido.csv\" #ej: nprnews-eeuu.csv, radionewzealand.csv\n",
    "df = pd.read_csv(DATASET,delimiter=\"|\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10000=df.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-1605dff17334>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mspacy_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTOP_WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "# preprocesar la columna \"text\" para tener un dataset de entrenamiento (lista de tokens)\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=\"\"\n",
    "\n",
    "for index,row in df_10000.iterrows():\n",
    "    # Text of the news\n",
    "    text=row[1]\n",
    "    \n",
    "    #preprocesamiento spacy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for token in doc:\n",
    "        #print(str(token).lower()+str(token.pos_))\n",
    "        if (str(token.pos_)!=\"SPACE\" and str(token.pos_)!=\"PUNCT\"):\n",
    "            train_dataset=train_dataset+str(token).lower()+\" \"\n",
    "\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "file = codecs.open(\"train_dataset.txt\", \"w\", \"utf-8\")\n",
    "file.write(train_dataset)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_dataset.txt', 'w') as file: # Use file to refer to the file object\n",
    "    file.write(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training word2vec\n",
    "\n",
    "tokens = word2vec.Text8Corpus('train_dataset.txt')\n",
    "\n",
    "model = word2vec.Word2Vec(tokens,size=200,hs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['woman'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['man'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"itv_reinounido_wordembedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
